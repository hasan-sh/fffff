{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7158ee2-ba3e-4ec2-a711-2f6cd6530de9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5468 > 512). Running this sequence through the model will result in indexing errors\n",
      "Downloading (…)\"pytorch_model.bin\";:  21%|▋  | 94.4M/440M [02:57<10:51, 532kB/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (5468) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 5468].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtextrecord\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 29\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(embedding)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Cluster the embeddings using K-Means\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mgenerate_embedding\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Feed the input tensor through the BERT model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m     last_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Extract the embedding for the [CLS] token\u001b[39;00m\n\u001b[1;32m     22\u001b[0m cls_embedding \u001b[38;5;241m=\u001b[39m last_hidden_states[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:985\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    984\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[0;32m--> 985\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (5468) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 5468].  Tensor sizes: [1, 512]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load the CSV file into a Pandas dataframe\n",
    "df = pd.read_csv('data/fpsc_relevant_ocm.csv')\n",
    "\n",
    "# Load the pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Define a function to generate embeddings for a given text\n",
    "def generate_embedding(text):\n",
    "    # Tokenize the text\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    # Convert the input to a tensor\n",
    "    input_tensor = torch.tensor([input_ids])\n",
    "    # Feed the input tensor through the BERT model\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_tensor)[0]\n",
    "    # Extract the embedding for the [CLS] token\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "    # Return the embedding as a NumPy array\n",
    "    return cls_embedding.numpy()\n",
    "\n",
    "# Generate embeddings for each text field in the dataframe\n",
    "embeddings = []\n",
    "for text in df['textrecord']:\n",
    "    embedding = generate_embedding(text)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# Cluster the embeddings using K-Means\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Extract SPOs from the clustered texts\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "spos = []\n",
    "for cluster in set(cluster_labels):\n",
    "    cluster_texts = df[cluster_labels == cluster]['text']\n",
    "    cluster_doc = nlp(' '.join(cluster_texts))\n",
    "    for sent in cluster_doc.sents:\n",
    "        sent_text = sent.text\n",
    "        sent_embedding = generate_embedding(sent_text)\n",
    "        # Perform SPO extraction here and append to spos list\n",
    "        spos.append((subject, predicate, object))\n",
    "\n",
    "# Print the extracted SPOs\n",
    "for spo in spos:\n",
    "    print(spo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7747552-a193-49eb-83ae-e918d97a7a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the text to process\n",
    "text = \"John hit the ball with a bat. Paris is the capital of France\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73599c20-0be7-4600-a4d7-26968ea1c4d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting allennlp\n",
      "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting allennlp-models\n",
      "  Downloading allennlp_models-2.10.1-py3-none-any.whl (464 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.5/464.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting lmdb>=1.2.1\n",
      "  Downloading lmdb-1.4.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (306 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.5/306.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4.0.0,>=3.12.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp) (3.19.6)\n",
      "Collecting pytest>=6.2.5\n",
      "  Downloading pytest-7.2.2-py3-none-any.whl (317 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.0.16 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp) (0.12.1)\n",
      "Collecting fairscale==0.4.6\n",
      "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorboardX>=1.2\n",
      "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock<3.8,>=3.3\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typer>=0.4.1 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp) (0.7.0)\n",
      "Requirement already satisfied: requests>=2.28 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp) (2.28.2)\n",
      "Collecting jsonnet>=0.10.0\n",
      "  Downloading jsonnet-0.19.1.tar.gz (593 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.6/593.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: h5py>=3.6.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp) (3.8.0)\n",
      "Collecting cached-path<1.2.0,>=1.1.3\n",
      "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
      "Collecting torchvision<0.14.0,>=0.8.1\n",
      "  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl (19.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp) (3.8.1)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp) (1.10.1)\n",
      "Collecting termcolor==1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting base58>=2.1.1\n",
      "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp) (0.3.6)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp) (1.2.1)\n",
      "Collecting torch<1.13.0,>=1.10.0\n",
      "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m489.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Using cached sacremoses-0.0.53.tar.gz (880 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<4.21,>=4.1\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Requirement already satisfied: numpy>=1.21.4 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp) (1.24.2)\n",
      "Requirement already satisfied: traitlets>5.1.1 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp) (5.9.0)\n",
      "Collecting wandb<0.13.0,>=0.10.0\n",
      "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp) (4.65.0)\n",
      "Collecting more-itertools>=8.12.0\n",
      "  Downloading more_itertools-9.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m868.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m156.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy<3.4,>=2.1.0\n",
      "  Downloading spacy-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting sentencepiece>=0.1.96\n",
      "  Using cached sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: datasets in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from allennlp-models) (2.10.1)\n",
      "Collecting word2number>=1.1\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Collecting py-rouge==1.1\n",
      "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting conllu==4.4.2\n",
      "  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
      "Collecting rich<13.0,>=12.1\n",
      "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.0.16\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-storage<3.0,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.2/110.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting boto3<2.0,>=1.0\n",
      "  Downloading boto3-1.26.92-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.7/134.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from huggingface-hub>=0.0.16->allennlp) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from huggingface-hub>=0.0.16->allennlp) (4.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from huggingface-hub>=0.0.16->allennlp) (6.0)\n",
      "Requirement already satisfied: joblib in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from nltk>=3.6.5->allennlp) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from nltk>=3.6.5->allennlp) (2022.10.31)\n",
      "Requirement already satisfied: click in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from nltk>=3.6.5->allennlp) (8.1.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from pytest>=6.2.5->allennlp) (22.2.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Using cached pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc8\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from requests>=2.28->allennlp) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from requests>=2.28->allennlp) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from requests>=2.28->allennlp) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from requests>=2.28->allennlp) (2022.12.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from scikit-learn>=1.0.1->allennlp) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (67.0.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
      "Collecting typer>=0.4.1\n",
      "  Using cached typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.8)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: jinja2 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.2)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (6.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.4.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.4)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.16.0)\n",
      "Requirement already satisfied: aiohttp in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from datasets->allennlp-models) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from datasets->allennlp-models) (11.0.0)\n",
      "Requirement already satisfied: xxhash in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from datasets->allennlp-models) (3.2.0)\n",
      "Requirement already satisfied: pandas in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from datasets->allennlp-models) (1.5.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from datasets->allennlp-models) (2023.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from datasets->allennlp-models) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from datasets->allennlp-models) (0.18.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from ftfy->allennlp-models) (0.2.6)\n",
      "Collecting botocore<1.30.0,>=1.29.92\n",
      "  Downloading botocore-1.29.92-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from aiohttp->datasets->allennlp-models) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from aiohttp->datasets->allennlp-models) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from aiohttp->datasets->allennlp-models) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from aiohttp->datasets->allennlp-models) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from aiohttp->datasets->allennlp-models) (1.3.3)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.16.2)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from pandas->datasets->allennlp-models) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from pandas->datasets->allennlp-models) (2022.7.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.0/223.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.3.0)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.8)\n",
      "Building wheels for collected packages: fairscale, termcolor, jsonnet, word2number, sacremoses, promise, pathtools\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307224 sha256=af4caef23dd3f0ddac63cfc9e7d85e94ff5cd60fd81cc6b53afc9d09d459172f\n",
      "  Stored in directory: /home/hasan-sh/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=57ca9619428874acf27bb46dc08142d7d5c79504f82f55d19334689b0e8229c3\n",
      "  Stored in directory: /home/hasan-sh/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jsonnet: filename=jsonnet-0.19.1-cp310-cp310-linux_x86_64.whl size=6413062 sha256=37874e111a44e7bb25f00ab4ab4e8e3d32340ff44c20726cb9a0c14e27e43798\n",
      "  Stored in directory: /home/hasan-sh/.cache/pip/wheels/19/0a/61/cf42e8e5c769072476554864d52898a609c9ccb7bbdf71a500\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5566 sha256=31537070912964678fef62c6ef995274b947e81e23bf4f2b2456d51445604246\n",
      "  Stored in directory: /home/hasan-sh/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=e0c61e09f3d94b2c247e08fe459323d90b07a19d14b7ddea3f4786681b2aec19\n",
      "  Stored in directory: /home/hasan-sh/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=543fd66b5e171291c388fa634463b90cc41ca0f31b1cd10595aacc80eb0ac168\n",
      "  Stored in directory: /home/hasan-sh/.cache/pip/wheels/54/4e/28/3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=91baaa292cfdf77e690ecdda97a3201c4fae0c258e125cccb59f6ba212d7ba2b\n",
      "  Stored in directory: /home/hasan-sh/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built fairscale termcolor jsonnet word2number sacremoses promise pathtools\n",
      "Installing collected packages: word2number, wasabi, tokenizers, termcolor, sentencepiece, py-rouge, pathtools, lmdb, jsonnet, commonmark, typer, torch, tensorboardX, smmap, shortuuid, setproctitle, sentry-sdk, sacremoses, rich, pydantic, promise, pluggy, more-itertools, jmespath, iniconfig, googleapis-common-protos, google-crc32c, ftfy, filelock, exceptiongroup, docker-pycreds, conllu, base58, torchvision, thinc, pytest, huggingface-hub, google-resumable-media, gitdb, fairscale, botocore, transformers, spacy, s3transfer, google-api-core, GitPython, wandb, google-cloud-core, boto3, google-cloud-storage, cached-path, allennlp, allennlp-models\n",
      "  Attempting uninstall: wasabi\n",
      "    Found existing installation: wasabi 1.1.1\n",
      "    Uninstalling wasabi-1.1.1:\n",
      "      Successfully uninstalled wasabi-1.1.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.2.0\n",
      "    Uninstalling termcolor-2.2.0:\n",
      "      Successfully uninstalled termcolor-2.2.0\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.7.0\n",
      "    Uninstalling typer-0.7.0:\n",
      "      Successfully uninstalled typer-0.7.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.6\n",
      "    Uninstalling pydantic-1.10.6:\n",
      "      Successfully uninstalled pydantic-1.10.6\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.9.0\n",
      "    Uninstalling filelock-3.9.0:\n",
      "      Successfully uninstalled filelock-3.9.0\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.9\n",
      "    Uninstalling thinc-8.1.9:\n",
      "      Successfully uninstalled thinc-8.1.9\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.12.1\n",
      "    Uninstalling huggingface-hub-0.12.1:\n",
      "      Successfully uninstalled huggingface-hub-0.12.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.26.1\n",
      "    Uninstalling transformers-4.26.1:\n",
      "      Successfully uninstalled transformers-4.26.1\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.5.1\n",
      "    Uninstalling spacy-3.5.1:\n",
      "      Successfully uninstalled spacy-3.5.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed GitPython-3.1.31 allennlp-2.10.1 allennlp-models-2.10.1 base58-2.1.1 boto3-1.26.92 botocore-1.29.92 cached-path-1.1.6 commonmark-0.9.1 conllu-4.4.2 docker-pycreds-0.4.0 exceptiongroup-1.1.1 fairscale-0.4.6 filelock-3.7.1 ftfy-6.1.1 gitdb-4.0.10 google-api-core-2.11.0 google-cloud-core-2.3.2 google-cloud-storage-2.7.0 google-crc32c-1.5.0 google-resumable-media-2.4.1 googleapis-common-protos-1.58.0 huggingface-hub-0.10.1 iniconfig-2.0.0 jmespath-1.0.1 jsonnet-0.19.1 lmdb-1.4.0 more-itertools-9.1.0 pathtools-0.1.2 pluggy-1.0.0 promise-2.3 py-rouge-1.1 pydantic-1.8.2 pytest-7.2.2 rich-12.6.0 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97 sentry-sdk-1.16.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 spacy-3.3.2 tensorboardX-2.6 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.20.1 typer-0.4.2 wandb-0.12.21 wasabi-0.10.1 word2number-1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install allennlp allennlp-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6218e2c9-e55f-4e06-bb48-3a709953cec6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[nltk_data] Downloading package punkt to /home/hasan-sh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/hasan-sh/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages/rich/live.py:229: UserWarning: install \"ipywidgets\"\n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/hasan-sh/.virtualenvs/thesis/lib/python3.10/site-packages/rich/live.py:229: UserWarning: install \"ipywidgets\"\n",
       "for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)/main/tokenizer.json: 100%|███| 466k/466k [00:00<00:00, 1.14MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|███| 440M/440M [00:36<00:00, 12.1MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': 'Did',\n",
       "   'description': '[V: Did] Uriah honestly think he could beat the game in under three hours ? .',\n",
       "   'tags': ['B-V',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O']},\n",
       "  {'verb': 'think',\n",
       "   'description': 'Did [ARG0: Uriah] [ARGM-ADV: honestly] [V: think] [ARG1: he could beat the game in under three hours] ? .',\n",
       "   'tags': ['O',\n",
       "    'B-ARG0',\n",
       "    'B-ARGM-ADV',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'I-ARG1',\n",
       "    'O',\n",
       "    'O']},\n",
       "  {'verb': 'could',\n",
       "   'description': 'Did Uriah honestly think he [V: could] beat the game in under three hours ? .',\n",
       "   'tags': ['O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'B-V',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O']},\n",
       "  {'verb': 'beat',\n",
       "   'description': 'Did Uriah honestly think [ARG0: he] [ARGM-MOD: could] [V: beat] [ARG1: the game] [ARGM-TMP: in under three hours] ? .',\n",
       "   'tags': ['O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'B-ARG0',\n",
       "    'B-ARGM-MOD',\n",
       "    'B-V',\n",
       "    'B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'B-ARGM-TMP',\n",
       "    'I-ARGM-TMP',\n",
       "    'I-ARGM-TMP',\n",
       "    'I-ARGM-TMP',\n",
       "    'O',\n",
       "    'O']}],\n",
       " 'words': ['Did',\n",
       "  'Uriah',\n",
       "  'honestly',\n",
       "  'think',\n",
       "  'he',\n",
       "  'could',\n",
       "  'beat',\n",
       "  'the',\n",
       "  'game',\n",
       "  'in',\n",
       "  'under',\n",
       "  'three',\n",
       "  'hours',\n",
       "  '?',\n",
       "  '.']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")\n",
    "predictor.predict(\n",
    "    sentence=\"Did Uriah honestly think he could beat the game in under three hours?.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "600d760c-fe56-4a20-9522-d085c2fd37fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, role \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(roles):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m role \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-ARG0\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# print(sent, dir(srl_results[\"words\"][i]))\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m         subj \u001b[38;5;241m=\u001b[39m sent[\u001b[43msrl_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstartswith\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:srl_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m][i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendswith\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m role \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-ARG1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     24\u001b[0m         obj \u001b[38;5;241m=\u001b[39m sent[srl_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m][i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartswith\u001b[39m\u001b[38;5;124m\"\u001b[39m]:srl_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m][i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendswith\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply NLP preprocessing\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define a list to hold SPOs\n",
    "spos = []\n",
    "\n",
    "# Iterate over the sentences in the text\n",
    "for sent in doc.sents:\n",
    "    # Get the SRL predictions for the sentence\n",
    "    srl_results = predictor.predict_tokenized(tokenized_sentence=[t.text for t in sent])\n",
    "    # Iterate over the SRL predictions for each predicate in the sentence\n",
    "    for predicate_index, predicate in enumerate(srl_results[\"verbs\"]):\n",
    "        # Get the predicate text and its semantic roles\n",
    "        predicate_text = predicate[\"verb\"]\n",
    "        roles = predicate[\"tags\"]\n",
    "        # Find the ARG0 and ARG1 roles\n",
    "        subj = \"\"\n",
    "        obj = \"\"\n",
    "        for i, role in enumerate(roles):\n",
    "            if role == \"B-ARG0\":\n",
    "                # print(sent, dir(srl_results[\"words\"][i]))\n",
    "                subj = sent[srl_results[\"words\"][i][\"startswith\"]:srl_results[\"words\"][i][\"endswith\"]]\n",
    "            elif role == \"B-ARG1\":\n",
    "                obj = sent[srl_results[\"words\"][i][\"startswith\"]:srl_results[\"words\"][i][\"endswith\"]]\n",
    "        # Add the SPO to the list\n",
    "        spos.append((subj.text, predicate_text, obj.text))\n",
    "\n",
    "# Print the generated SPOs\n",
    "for spo in spos:\n",
    "    print(spo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cacddd63-a2b5-4e16-9ddd-d56b5e09e697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': 'is',\n",
       "   'description': '[ARG1: My name] [V: is] [ARG2: Hasan] and I am 27 years old .',\n",
       "   'tags': ['B-ARG1',\n",
       "    'I-ARG1',\n",
       "    'B-V',\n",
       "    'B-ARG2',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O']},\n",
       "  {'verb': 'am',\n",
       "   'description': 'My name is Hasan and [ARG1: I] [V: am] [ARG2: 27 years old] .',\n",
       "   'tags': ['O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'B-ARG1',\n",
       "    'B-V',\n",
       "    'B-ARG2',\n",
       "    'I-ARG2',\n",
       "    'I-ARG2',\n",
       "    'O']}],\n",
       " 'words': ['My',\n",
       "  'name',\n",
       "  'is',\n",
       "  'Hasan',\n",
       "  'and',\n",
       "  'I',\n",
       "  'am',\n",
       "  '27',\n",
       "  'years',\n",
       "  'old',\n",
       "  '.']}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(\n",
    "    sentence=\"My name is Hasan and I am 27 years old.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7b12e75-e3ba-44c6-852c-5372221167e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(John, Paris, France)\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents)#[0].label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
