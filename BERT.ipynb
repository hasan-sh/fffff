{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0ea42-6028-488a-bc1e-18f105dd604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5809142-d50f-4c2b-b3de-fa4887cec16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess data and tokenize text records\n",
    "def preprocess_data(data):\n",
    "    # Preprocess data (e.g., remove missing values)\n",
    "    # ...\n",
    "\n",
    "    # Tokenize text records using BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    encoded_data = tokenizer(data['text_record'].tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_mask = encoded_data['attention_mask']\n",
    "\n",
    "    labels = data['ocms'].tolist()\n",
    "    labels = [int(label) for label in labels]\n",
    "\n",
    "    return input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c8f22-2915-4a11-b8ad-694ff8385982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train a BERT model on the data\n",
    "def train_model(input_ids, attention_mask, labels):\n",
    "    # Split data into training and validation sets\n",
    "    train_input_ids, val_input_ids, train_attention_mask, val_attention_mask, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2)\n",
    "\n",
    "    # Load BERT model and set up optimizer and loss function\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train BERT model\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(train_input_ids, train_attention_mask, labels=train_labels)\n",
    "        loss = loss_fn(outputs.logits, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(val_input_ids, val_attention_mask)\n",
    "            val_loss = loss_fn(outputs.logits, val_labels)\n",
    "            val_acc = (outputs.logits.argmax(1) == val_labels).float().mean()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: validation loss = {val_loss:.4f}, validation accuracy = {val_acc:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d889b-f336-4930-929d-33d517995a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to make predictions using the trained BERT model\n",
    "def predict(model, text_records):\n",
    "    # Tokenize new text records using BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    encoded_data = tokenizer(text_records, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_mask = encoded_data['attention_mask']\n",
    "\n",
    "    # Make predictions using the trained BERT model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = outputs.logits.argmax(1).tolist()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Preprocess data and tokenize text records\n",
    "input_ids, attention_mask, labels = preprocess_data(data)\n",
    "\n",
    "# Train a BERT model on the data\n",
    "model = train_model(input_ids, attention_mask, labels)\n",
    "\n",
    "# Make predictions on new data\n",
    "new_data = ['text record 1', 'text record 2', 'text record 3']\n",
    "predictions = predict(model, new_data)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
